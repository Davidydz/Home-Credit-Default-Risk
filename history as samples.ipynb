{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca25a2f95f0fc9498b9b2e3a9d96607fbb682015"
   },
   "source": [
    "# Home Credit Default Risk 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2776978fa169449d5bf3c8f036668afe10a47502"
   },
   "source": [
    "__Warning!__ This kernel cannot run on Kaggle: not enough memory. I run everything on Google Datalab with 8 core highmem cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e45783c7652aae4607fc8c1ac79c979d7b387043"
   },
   "source": [
    "Based on the following kernel: \n",
    "\n",
    "- https://www.kaggle.com/aantonova/preliminary-analysis-of-application-dataset/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "cc4088625ae2209899d05c70dfd7bcb108cb4c3a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import google.datalab.storage as storage\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "be294207f4e12ccf54d922814789b27998577dca"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import ranksums\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32f2b23ed9dabb4b8ec59cc86f1e9cecdc9abad3"
   },
   "source": [
    "## Aggregating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3687d746e5fe89d3477cff12521e1402431c7ab1"
   },
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "4b1d9d1291ce3da6dc089ec6c3918f485528caf0"
   },
   "outputs": [],
   "source": [
    "#Reduce the storage used by the dataset\n",
    "def reduce_mem_usage(data, verbose = True):\n",
    "    \"\"\"\n",
    "    data: the original dataset\n",
    "    verbose: if true, show the information of the data set after memory usage reduced\n",
    "    \n",
    "    The function returns a copy of the original dataset which uses less memory\n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            #assign integer type based on its magnitude\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            #assign floating number type based on its magnitude\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each categorical column, transform every category to an integer.\n",
    "def cat_to_num(data, nan_as_category = True):\n",
    "    \"\"\"\n",
    "    data : original data or data with reduced memory usage\n",
    "    nan_as_category : Default is true, it will treat missing value as a new category.\n",
    "    \n",
    "    The function \n",
    "    \"\"\"\n",
    "    categorical_columns = [col for col in data.columns \\\n",
    "                        if not pd.api.types.is_numeric_dtype(data[col].dtype)]\n",
    "    for c in categorical_columns:\n",
    "        data.loc[:,c] = data.loc[:,c].astype(\"category\")\n",
    "        #fill na with categorical value\n",
    "        if sum(pd.isna(data.loc[:,c])) > 0 and nan_as_category:\n",
    "        data.loc[:,c] = data.loc[:,c].cat.add_categories(\"NaN\").fillna(\"NaN\")\n",
    "        #replace category with number\n",
    "        data.loc[:,c] = data.loc[:,c].cat.codes\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corr_col(data):\n",
    "    \"\"\"\n",
    "    The function adds correlation columns to the copy of the original dataset, \n",
    "    each correlation column is the multiplication of two selected features. And then return the \n",
    "    new dataset\n",
    "    \"\"\"\n",
    "    feats = [f for f in data.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    for i in range(len(feats)):\n",
    "    for j in range(i, len(feats)):\n",
    "        #The name of the correlation column is the combination of the names of the two selected features\n",
    "        name1 = feats[i]\n",
    "        name2 = feats[j]\n",
    "        new_name = name1 + \"_\" + name2\n",
    "        data[new_name] = data[name1].multiply(data[name2])\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the new features on client level\n",
    "def summarize_data(data, data_name, group_name, length_name):\n",
    "    \"\"\"\n",
    "    data : original data\n",
    "    data_name : name of the dataset\n",
    "    group_name : the column for client id\n",
    "    length_name : the name of the column that summarize the length of a client's history\n",
    "    The function aggregate the data on client level and return the aggregated dataset\n",
    "    \"\"\"\n",
    "    #Aggregate data on client level\n",
    "    data = cat_to_num(data)\n",
    "    data = add_corr_col(data)\n",
    "    aggregations = {}\n",
    "    data_agg = data.groupby(group_name).mean()\n",
    "    #Define the names of the aggregate data\n",
    "    new_col = [data_name + col for col in data_agg.columns]\n",
    "    data_agg.columns = new_col\n",
    "    data_agg[length_name] = data.groupby(group_name).size()\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return data_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2b6b0a9f460b76c27a9b9f3bf20731aaac77f19"
   },
   "source": [
    "### Aggregating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "b047b0a2701c2586e817a0a4acd4ce88c4b29d65"
   },
   "outputs": [],
   "source": [
    "def application_train_test(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    The function aggregates application_train.csv and application_test.csv together\n",
    "    and return the aggregated dataset\n",
    "    \"\"\"\n",
    "    # Read data and merge\n",
    "    temp_data1 = storage.Object('firstproject-214518', 'dataset/application_train.csv').read_stream()\n",
    "    df_train = pd.read_csv(BytesIO(temp_data1))\n",
    "    print(\"The dataset application_train has the shape {}\".format(df_train.shape))\n",
    "    temp_data2 = storage.Object('firstproject-214518', 'dataset/application_test.csv').read_stream()\n",
    "    df_test = pd.read_csv(BytesIO(temp_data2))\n",
    "    df = pd.concat([df_train, df_test], axis = 0, ignore_index = True)\n",
    "    del df_train, df_test, temp_data1, temp_data2\n",
    "    gc.collect()\n",
    "    \n",
    "    # Remove some rows with values not present in test set\n",
    "    df.drop(df[df['CODE_GENDER'] == 'XNA'].index, inplace = True)\n",
    "    df.drop(df[df['NAME_INCOME_TYPE'] == 'Maternity leave'].index, inplace = True)\n",
    "    df.drop(df[df['NAME_FAMILY_STATUS'] == 'Unknown'].index, inplace = True)\n",
    "    \n",
    "    # Remove some empty features\n",
    "    df.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n",
    "            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n",
    "            'FLAG_DOCUMENT_21'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Replace some outliers\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n",
    "    df.loc[df['OWN_CAR_AGE'] > 80, 'OWN_CAR_AGE'] = np.nan\n",
    "    df.loc[df['REGION_RATING_CLIENT_W_CITY'] < 0, 'REGION_RATING_CLIENT_W_CITY'] = np.nan\n",
    "    df.loc[df['AMT_INCOME_TOTAL'] > 1e8, 'AMT_INCOME_TOTAL'] = np.nan\n",
    "    df.loc[df['AMT_REQ_CREDIT_BUREAU_QRT'] > 10, 'AMT_REQ_CREDIT_BUREAU_QRT'] = np.nan\n",
    "    df.loc[df['OBS_30_CNT_SOCIAL_CIRCLE'] > 40, 'OBS_30_CNT_SOCIAL_CIRCLE'] = np.nan\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], _ = pd.factorize(df[bin_feature])\n",
    "        \n",
    "    # transform all categories in the categorical column as integers\n",
    "    df = cat_to_num(df, nan_as_category)\n",
    "    \n",
    "    # Some new features\n",
    "    df['app missing'] = df.isnull().sum(axis = 1).values\n",
    "    df['app AMT_CREDIT - AMT_GOODS_PRICE'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n",
    "    df['app AMT_CREDIT / AMT_GOODS_PRICE'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    df['app AMT_CREDIT / AMT_ANNUITY'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['app AMT_CREDIT / AMT_INCOME_TOTAL'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    \n",
    "    df['app AMT_INCOME_TOTAL / 12 - AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / 12. - df['AMT_ANNUITY']\n",
    "    df['app AMT_INCOME_TOTAL / AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] / df['AMT_ANNUITY']\n",
    "    df['app AMT_INCOME_TOTAL - AMT_GOODS_PRICE'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE']\n",
    "    df['app AMT_INCOME_TOTAL / CNT_FAM_MEMBERS'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['app AMT_INCOME_TOTAL / CNT_CHILDREN'] = df['AMT_INCOME_TOTAL'] / (1 + df['CNT_CHILDREN'])\n",
    "    \n",
    "    df['app most popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n",
    "                        .isin([225000, 450000, 675000, 900000]).map({True: 1, False: 0})\n",
    "    df['app popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n",
    "                        .isin([1125000, 1350000, 1575000, 1800000, 2250000]).map({True: 1, False: 0})\n",
    "    \n",
    "    df['app OWN_CAR_AGE / DAYS_BIRTH'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['app OWN_CAR_AGE / DAYS_EMPLOYED'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    \n",
    "    df['app DAYS_LAST_PHONE_CHANGE / DAYS_BIRTH'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    df['app DAYS_LAST_PHONE_CHANGE / DAYS_EMPLOYED'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    df['app DAYS_EMPLOYED - DAYS_BIRTH'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n",
    "    df['app DAYS_EMPLOYED / DAYS_BIRTH'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    df['app CNT_CHILDREN / CNT_FAM_MEMBERS'] = df['CNT_CHILDREN'] / df['CNT_FAM_MEMBERS']\n",
    "    gc.collect()\n",
    "    return reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "f7e1514251cc659151f83eaed84deb9559536e04"
   },
   "outputs": [],
   "source": [
    "def bureau_and_balance(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    This function aggregates datasets related with bureau and return the aggregated dataset \n",
    "    \"\"\"\n",
    "    #Read in the data and reduce its memory usage\n",
    "    temp_data1 = storage.Object('firstproject-214518', 'dataset/bureau_balance.csv').read_stream()\n",
    "    df_bureau_b = reduce_mem_usage(pd.read_csv(BytesIO(temp_data1)), verbose = False)\n",
    "    del temp_data1\n",
    "    print(\"The dataset bureau_balance has the shape {}\".format(df_bureau_b.shape))\n",
    "    #summarize all the information on client level\n",
    "    df_bureau_b_agg = summarize_data(df_bureau_b, \"bureau_balance\",\"SK_ID_BUREAU\", \"balance_hist_length\")\n",
    "    del df_bureau_b\n",
    "    gc.collect()\n",
    "    \n",
    "    temp_data2 = storage.Object('firstproject-214518', 'dataset/bureau.csv').read_stream()\n",
    "    df_bureau = reduce_mem_usage(pd.read_csv(BytesIO(temp_data2)),verbose = False)\n",
    "    del temp_data2\n",
    "    print(\"The dataset bureau has the shape {}\".format(df_bureau.shape))\n",
    "    # Replace\\remove some outliers in bureau set\n",
    "    df_bureau.loc[df_bureau['AMT_ANNUITY'] > .8e8, 'AMT_ANNUITY'] = np.nan\n",
    "    df_bureau.loc[df_bureau['AMT_CREDIT_SUM'] > 3e8, 'AMT_CREDIT_SUM'] = np.nan\n",
    "    df_bureau.loc[df_bureau['AMT_CREDIT_SUM_DEBT'] > 1e8, 'AMT_CREDIT_SUM_DEBT'] = np.nan\n",
    "    df_bureau.loc[df_bureau['AMT_CREDIT_MAX_OVERDUE'] > .8e8, 'AMT_CREDIT_MAX_OVERDUE'] = np.nan\n",
    "    df_bureau.loc[df_bureau['DAYS_ENDDATE_FACT'] < -10000, 'DAYS_ENDDATE_FACT'] = np.nan\n",
    "    df_bureau.loc[(df_bureau['DAYS_CREDIT_UPDATE'] > 0) | (df_bureau['DAYS_CREDIT_UPDATE'] < -40000), 'DAYS_CREDIT_UPDATE'] = np.nan\n",
    "    df_bureau.loc[df_bureau['DAYS_CREDIT_ENDDATE'] < -10000, 'DAYS_CREDIT_ENDDATE'] = np.nan\n",
    "    df_bureau.drop(df_bureau[df_bureau['DAYS_ENDDATE_FACT'] < df_bureau['DAYS_CREDIT']].index, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Bureau balance: merge with bureau.csv\n",
    "    df_bureau = df_bureau.join(df_bureau_b_agg, how = 'left', on = 'SK_ID_BUREAU')\n",
    "    df_bureau.drop('SK_ID_BUREAU', axis = 1, inplace = True)\n",
    "    del df_bureau_b_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    df_bureau_agg = summarize_data(df_bureau,\"bureau\", \"SK_ID_CURR\", \"bureau_hist_length\")\n",
    "    del df_bureau\n",
    "    gc.collect()\n",
    "    return reduce_mem_usage(df_bureau_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "611655246d527af034c24dae72e5cdfee3b404cb"
   },
   "outputs": [],
   "source": [
    "def previous_application(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    The function aggregates the credit history datasets together and return the aggregated dataset\n",
    "    \"\"\"\n",
    "    temp_data = storage.Object('firstproject-214518', 'dataset/previous_application.csv').read_stream()\n",
    "    df_prev = reduce_mem_usage(pd.read_csv(BytesIO(temp_data)))\n",
    "    del temp_data\n",
    "    print(\"The dataset previous_application has the shape {}\".format(df_prev.shape))\n",
    "    # Replace some outliers\n",
    "    df_prev.loc[df_prev['AMT_CREDIT'] > 6000000, 'AMT_CREDIT'] = np.nan\n",
    "    df_prev.loc[df_prev['SELLERPLACE_AREA'] > 3500000, 'SELLERPLACE_AREA'] = np.nan\n",
    "    df_prev[['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n",
    "             'DAYS_LAST_DUE', 'DAYS_TERMINATION']].replace(365243, np.nan, inplace = True)\n",
    "    # Some new features\n",
    "    df_prev['prev missing'] = df_prev.isnull().sum(axis = 1).values\n",
    "    df_prev['prev AMT_APPLICATION / AMT_CREDIT'] = df_prev['AMT_APPLICATION'] / df_prev['AMT_CREDIT']\n",
    "    df_prev['prev AMT_APPLICATION - AMT_CREDIT'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_CREDIT']\n",
    "    df_prev['prev AMT_APPLICATION - AMT_GOODS_PRICE'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_GOODS_PRICE']\n",
    "    df_prev['prev AMT_GOODS_PRICE - AMT_CREDIT'] = df_prev['AMT_GOODS_PRICE'] - df_prev['AMT_CREDIT']\n",
    "    df_prev['prev DAYS_FIRST_DRAWING - DAYS_FIRST_DUE'] = df_prev['DAYS_FIRST_DRAWING'] - df_prev['DAYS_FIRST_DUE']\n",
    "    df_prev['prev DAYS_TERMINATION less -500'] = (df_prev['DAYS_TERMINATION'] < -500).astype(int)\n",
    "    #summarize the data on client level\n",
    "    df_prev_agg = summarize_data(df_prev,\"previous\", \"SK_ID_CURR\", \"prev_app_hist_length\")\n",
    "    del df_prev\n",
    "    gc.collect()\n",
    "    return reduce_mem_usage(df_prev_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "eb88e53d8bc09a2d4be77ad31dfb60bb39d3cef6"
   },
   "outputs": [],
   "source": [
    "def pos_cash(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    The dataset aggregate the POS_CASH_balance.csv dataset on client level and return the aggregated dataset\n",
    "    \"\"\"\n",
    "    temp_data = storage.Object('firstproject-214518', 'dataset/POS_CASH_balance.csv').read_stream()\n",
    "    df_pos = reduce_mem_usage(pd.read_csv(BytesIO(temp_data)))\n",
    "    del temp_data\n",
    "    print(\"The dataset pos_cash_balance has the shape {}\".format(df_pos.shape))\n",
    "    # Replace some outliers\n",
    "    df_pos.loc[df_pos['CNT_INSTALMENT_FUTURE'] > 60, 'CNT_INSTALMENT_FUTURE'] = np.nan\n",
    "    \n",
    "    # some new features\n",
    "    df_pos['pos CNT_INSTALMENT more CNT_INSTALMENT_FUTURE'] = \\\n",
    "                    (df_pos['CNT_INSTALMENT'] > df_pos['CNT_INSTALMENT_FUTURE']).astype(int)\n",
    "    \n",
    "    # summarize the data on client level\n",
    "    df_pos_agg = summarize_data(df_pos,\"pos\", \"SK_ID_CURR\", \"pos_hist_length\")\n",
    "    del df_pos\n",
    "    gc.collect()\n",
    "    return reduce_mem_usage(df_pos_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "1270941b2152488627f025154b918d4b8e285576"
   },
   "outputs": [],
   "source": [
    "def installments_payments(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    The function aggregate the installments_payments.csv dataset, and return the aggregated one\n",
    "    \"\"\"\n",
    "    temp_data = storage.Object('firstproject-214518', 'dataset/installments_payments.csv').read_stream()\n",
    "    df_ins = reduce_mem_usage(pd.read_csv(BytesIO(temp_data)))\n",
    "    del temp_data\n",
    "    print(\"The datset installments_payments has the shape {}\".format(df_ins.shape))\n",
    "    # Replace some outliers\n",
    "    df_ins.loc[df_ins['NUM_INSTALMENT_VERSION'] > 70, 'NUM_INSTALMENT_VERSION'] = np.nan\n",
    "    df_ins.loc[df_ins['DAYS_ENTRY_PAYMENT'] < -4000, 'DAYS_ENTRY_PAYMENT'] = np.nan\n",
    "    \n",
    "    # Some new features\n",
    "    df_ins['ins DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT'] = df_ins['DAYS_ENTRY_PAYMENT'] - df_ins['DAYS_INSTALMENT']\n",
    "    df_ins['ins NUM_INSTALMENT_NUMBER_100'] = (df_ins['NUM_INSTALMENT_NUMBER'] == 100).astype(int)\n",
    "    df_ins['ins DAYS_INSTALMENT more NUM_INSTALMENT_NUMBER'] = (df_ins['DAYS_INSTALMENT'] > df_ins['NUM_INSTALMENT_NUMBER'] * 50 / 3 - 11500 / 3).astype(int)\n",
    "    df_ins['ins AMT_INSTALMENT - AMT_PAYMENT'] = df_ins['AMT_INSTALMENT'] - df_ins['AMT_PAYMENT']\n",
    "    df_ins['ins AMT_PAYMENT / AMT_INSTALMENT'] = df_ins['AMT_PAYMENT'] / df_ins['AMT_INSTALMENT']\n",
    "    #summarize the data on client level\n",
    "    df_ins_agg = summarize_data(df_ins,\"ins\", \"SK_ID_CURR\", \"ins_hist_length\") \n",
    "    del df_ins\n",
    "    gc.collect()  \n",
    "    return reduce_mem_usage(df_ins_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "abda66f62993adecb7b8e1a1286de38504312930"
   },
   "outputs": [],
   "source": [
    "def credit_card_balance(nan_as_category = True):\n",
    "    \"\"\"\n",
    "    Aggregate the credit card data and return it\n",
    "    \"\"\"\n",
    "    temp_data = storage.Object('firstproject-214518', 'dataset/credit_card_balance.csv').read_stream()\n",
    "    df_card = reduce_mem_usage(pd.read_csv(BytesIO(temp_data)))\n",
    "    print(\"The dataset credit_card_balance has the shape {}\".format(df_card.shape))\n",
    "    del temp_data\n",
    "    # Replace some outliers\n",
    "    df_card.loc[df_card['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\n",
    "    df_card.loc[df_card['AMT_CREDIT_LIMIT_ACTUAL'] > 1000000, 'AMT_CREDIT_LIMIT_ACTUAL'] = np.nan\n",
    "     # Some new features\n",
    "    df_card['card missing'] = df_card.isnull().sum(axis = 1).values\n",
    "    df_card['card SK_DPD - MONTHS_BALANCE'] = df_card['SK_DPD'] - df_card['MONTHS_BALANCE']\n",
    "    df_card['card SK_DPD_DEF - MONTHS_BALANCE'] = df_card['SK_DPD_DEF'] - df_card['MONTHS_BALANCE']\n",
    "    df_card['card SK_DPD - SK_DPD_DEF'] = df_card['SK_DPD'] - df_card['SK_DPD_DEF']\n",
    "  \n",
    "    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECIVABLE'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECIVABLE']\n",
    "    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n",
    "\n",
    "    df_card['card AMT_RECIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_RECIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n",
    "\n",
    "    df_card['card AMT_BALANCE - AMT_RECIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_RECIVABLE']\n",
    "    df_card['card AMT_BALANCE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_BALANCE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n",
    "    df_card['card AMT_BALANCE - AMT_TOTAL_RECEIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_TOTAL_RECEIVABLE']\n",
    "\n",
    "    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_ATM_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_ATM_CURRENT']\n",
    "    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_OTHER_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_OTHER_CURRENT']\n",
    "    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_POS_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_POS_CURRENT']\n",
    "    \n",
    "    df_card_agg = summarize_data(df_card,\"card\", \"SK_ID_CURR\", \"credit_hist_length\")\n",
    "    \n",
    "    del df_card\n",
    "    gc.collect()\n",
    "    \n",
    "    return reduce_mem_usage(df_card_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "390cbb9f2068e8feef6aac66a797c54697924d63"
   },
   "outputs": [],
   "source": [
    "def aggregate():\n",
    "    \"\"\"\n",
    "    This function merge(left join) all datasets together and return \n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action = 'ignore')\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('1: application train & test (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    df = application_train_test()\n",
    "    print('     DF shape:', df.shape)   \n",
    "    print('-' * 20)\n",
    "    print('2: bureau & balance (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    bureau = bureau_and_balance()\n",
    "    df = df.join(bureau, how = 'left', on = 'SK_ID_CURR')\n",
    "    print('     DF shape:', df.shape)\n",
    "    del bureau\n",
    "    gc.collect()\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('3: previous_application (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    prev = previous_application()\n",
    "    df = df.join(prev, how = 'left', on = 'SK_ID_CURR')\n",
    "    print('     DF shape:', df.shape)\n",
    "    del prev\n",
    "    gc.collect()\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('4: POS_CASH_balance (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    pos = pos_cash()\n",
    "    df = df.join(pos, how = 'left', on = 'SK_ID_CURR')\n",
    "    print('     DF shape:', df.shape)\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('5: installments_payments (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    ins = installments_payments()\n",
    "    df = df.join(ins, how = 'left', on = 'SK_ID_CURR')\n",
    "    print('     DF shape:', df.shape)\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('6: credit_card_balance (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    cc = credit_card_balance()\n",
    "    df = df.join(cc, how = 'left', on = 'SK_ID_CURR')\n",
    "    print('     DF shape:', df.shape)\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    \n",
    "    print('-' * 20)\n",
    "    print('7: final dataset (', time.ctime(), ')')\n",
    "    print('-' * 20)\n",
    "    return reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cdf22861b989e1dfe6331ecef61b13ef3fa2d05"
   },
   "source": [
    "## Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "c8536f211c9768b6f7622a43413e8bdd2a46092d"
   },
   "outputs": [],
   "source": [
    "def corr_feature_with_target(feature, target):\n",
    "    \"\"\"\n",
    "    target : y value\n",
    "    feature: the feature of the data set\n",
    "    \n",
    "    This function calculates the correlation between feature and target, and return the Wilcoxon rank-sum statistic\n",
    "    as well as the \n",
    "    \n",
    "    \"\"\"\n",
    "    c0 = feature[target == 0].dropna()\n",
    "    c1 = feature[target == 1].dropna()\n",
    "    \n",
    "    #For binary features, calculate mean difference\n",
    "    #For continuous features, calculate median difference\n",
    "    if set(feature.unique()) == set([0, 1]):\n",
    "        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n",
    "    else:\n",
    "        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n",
    "    #When sample size >= 20, use Wilcoxon rank-sum statistic\n",
    "    #otherwise set it to be 2\n",
    "    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n",
    "        \n",
    "    return [diff, p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "c04f8067c2a3971de8cc0bd521732e18b78ee4c5"
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Clean the data and return it\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(action = 'ignore')\n",
    "    \n",
    "    # Removing empty features\n",
    "    nun = data.nunique()\n",
    "    empty = list(nun[nun <= 1].index)\n",
    "    \n",
    "    data.drop(empty, axis = 1, inplace = True)\n",
    "    print('After removing empty features there are {0:d} features'.format(data.shape[1]))\n",
    "    \n",
    "    # Removing features with the same distribution on 0 and 1 classes\n",
    "    corr = pd.DataFrame(index = ['diff', 'p'])\n",
    "    ind = data[data['TARGET'].notnull()].index\n",
    "    \n",
    "    for c in data.columns.drop('TARGET'):\n",
    "        corr[c] = corr_feature_with_target(data.loc[ind, c].astype(\"float64\"), data.loc[ind, 'TARGET'].astype(\"float64\"))\n",
    "    \n",
    "    corr = corr.T\n",
    "    gc.collect()\n",
    "    corr['diff_norm'] = abs(corr['diff'] / data.mean(axis = 0))\n",
    "    #if correlation is not signifcant, delete it\n",
    "    to_del_1 = corr[((corr['diff'] == 0) & (corr['p'] > .05))].index\n",
    "    to_del_2 = corr[((corr['diff_norm'] < .5) & (corr['p'] > .05))].drop(to_del_1).index\n",
    "    to_del = list(to_del_1) + list(to_del_2)\n",
    "    if 'SK_ID_CURR' in to_del:\n",
    "        to_del.remove('SK_ID_CURR')\n",
    "        \n",
    "    data.drop(to_del, axis = 1, inplace = True)\n",
    "    print('After removing features with the same distribution on 0 and 1 classes there are {0:d} features'.format(data.shape[1]))\n",
    "    \n",
    "    # Removing features with not the same distribution on train and test datasets\n",
    "    corr_test = pd.DataFrame(index = ['diff', 'p'])\n",
    "    target = data['TARGET'].notnull().astype(int)\n",
    "    \n",
    "    for c in data.columns.drop('TARGET'):\n",
    "        corr_test[c] = corr_feature_with_target(data[c].astype(\"float64\"), target.astype(\"float64\"))\n",
    "\n",
    "    corr_test = corr_test.T\n",
    "    gc.collect()\n",
    "    corr_test['diff_norm'] = abs(corr_test['diff'] / data.mean(axis = 0))\n",
    "    \n",
    "    bad_features = corr_test[((corr_test['p'] < .05) & (corr_test['diff_norm'] > 1))].index\n",
    "    bad_features = corr.loc[bad_features][corr['diff_norm'] == 0].index\n",
    "    \n",
    "    data.drop(bad_features, axis = 1, inplace = True)\n",
    "    print('After removing features with not the same distribution on train and test datasets there are {0:d} features'.format(data.shape[1]))\n",
    "    \n",
    "    del corr, corr_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Removing features not interesting for classifier\n",
    "    clf = LGBMClassifier(random_state = 0)\n",
    "    train_index = data[data['TARGET'].notnull()].index\n",
    "    train_columns = data.drop('TARGET', axis = 1).columns\n",
    "\n",
    "    score = 1\n",
    "    new_columns = []\n",
    "    while score > .7:\n",
    "        train_columns = train_columns.drop(new_columns)\n",
    "        clf.fit(data.loc[train_index, train_columns], data.loc[train_index, 'TARGET'])\n",
    "        f_imp = pd.Series(clf.feature_importances_, index = train_columns)\n",
    "        score = roc_auc_score(data.loc[train_index, 'TARGET'], \n",
    "                              clf.predict_proba(data.loc[train_index, train_columns])[:, 1])\n",
    "        new_columns = f_imp[f_imp > 0].index\n",
    "\n",
    "    data.drop(train_columns, axis = 1, inplace = True)\n",
    "    print('After removing features not interesting for classifier there are {0:d} features'.format(data.shape[1]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f7495063700c9e611ea1e27f90f40acd5085cae"
   },
   "source": [
    "## Optimization LGBM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9148f2a806be7e22a5cfb90442d82a72e1da4084"
   },
   "source": [
    "### Optimization and visualisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "b440750e95bc79ad0603245387b08aefe048b0af"
   },
   "outputs": [],
   "source": [
    "def cv_scores(df, num_folds, params, stratified = False, verbose = -1, \n",
    "              save_train_prediction = True, train_prediction_file_name = 'train_prediction1.csv',\n",
    "              save_test_prediction = True, test_prediction_file_name = 'test_prediction1.csv'):\n",
    "    \"\"\"\n",
    "    df : the final dataset after aggregation and cleaning\n",
    "    num_folds: number of folds for cross validation\n",
    "    params: parameters for the LightGBM classifier\n",
    "    stratified : whether to use stratified sampling or not\n",
    "    verbose : whether to print model message or not while fitting model\n",
    "    save_train_prediction : whether to save the prediction on training data or not\n",
    "    train_prediction_file_name : the name of the file that stores the prediction on training set\n",
    "    save_test_prediction : whether to save the prediction on the test dataset or not\n",
    "    test_prediction_file_name : the name of the file that stores the prediction on the test dataset\n",
    "    \n",
    "    The function stores the final result and return the score on both training and testing dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    clf = LGBMClassifier(**params)\n",
    "\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n",
    "        \n",
    "    # Create arrays and dataframes to store results\n",
    "    train_pred = np.zeros(train_df.shape[0])\n",
    "    train_pred_proba = np.zeros(train_df.shape[0])\n",
    "\n",
    "    test_pred = np.zeros(train_df.shape[0])\n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    prediction = np.zeros(test_df.shape[0])\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    df_feature_importance = pd.DataFrame(index = feats)\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        print('Fold', n_fold, 'started at', time.ctime())\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = verbose, early_stopping_rounds = 200)\n",
    "\n",
    "        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n",
    "        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        prediction += \\\n",
    "                clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        df_feature_importance[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, test_pred_proba[valid_idx])))\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    roc_auc_train = roc_auc_score(train_df['TARGET'], train_pred_proba)\n",
    "    precision_train = precision_score(train_df['TARGET'], train_pred, average = None)\n",
    "    recall_train = recall_score(train_df['TARGET'], train_pred, average = None)\n",
    "    \n",
    "    roc_auc_test = roc_auc_score(train_df['TARGET'], test_pred_proba)\n",
    "    precision_test = precision_score(train_df['TARGET'], test_pred, average = None)\n",
    "    recall_test = recall_score(train_df['TARGET'], test_pred, average = None)\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_test)\n",
    "    \n",
    "    df_feature_importance.fillna(0, inplace = True)\n",
    "    df_feature_importance['mean'] = df_feature_importance.mean(axis = 1)\n",
    "    \n",
    "    # Write prediction files\n",
    "    if save_train_prediction:\n",
    "        df_prediction = train_df[['SK_ID_CURR', 'TARGET']]\n",
    "        df_prediction['Prediction'] = test_pred_proba\n",
    "        df_prediction.to_csv(train_prediction_file_name, index = False)\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "\n",
    "    if save_test_prediction:\n",
    "        df_prediction = test_df[['SK_ID_CURR']]\n",
    "        df_prediction['TARGET'] = prediction\n",
    "        df_prediction.to_csv(test_prediction_file_name, index = False)\n",
    "        del df_prediction\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_feature_importance, \\\n",
    "           [roc_auc_train, roc_auc_test,\n",
    "            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n",
    "            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "9c8cd095e8a6472475c239973fc25af25e10534e"
   },
   "outputs": [],
   "source": [
    "def display_folds_importances(feature_importance_df_, n_folds = 5):\n",
    "    n_columns = 3\n",
    "    n_rows = (n_folds + 1) // n_columns\n",
    "    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n",
    "    for i in range(n_folds):\n",
    "        sns.barplot(x = i, y = 'index', data = feature_importance_df_.reset_index().sort_values(i, ascending = False).head(20), \n",
    "                    ax = axes[i // n_columns, i % n_columns])\n",
    "    sns.barplot(x = 'mean', y = 'index', data = feature_importance_df_.reset_index().sort_values('mean', ascending = False).head(20), \n",
    "                    ax = axes[n_rows - 1, n_columns - 1])\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warninig: parameters are not fully optimized due to the limit of time\n",
    "# The parameter below is based on a little bit different dataset\n",
    "# Hyper parameters are obtained using Bayesian-Optimization\n",
    "lgbm_params = {\n",
    "            'nthread': 4,\n",
    "            'n_estimators': 10000,\n",
    "            'learning_rate': 0.02,\n",
    "            'num_leaves': 33,\n",
    "            'colsample_bytree': 0.805415337217056,\n",
    "            'subsample': 0.8082978617542438,\n",
    "            'max_depth': 8,\n",
    "            'reg_alpha': 0.04988371263522236,\n",
    "            'reg_lambda': 0.0793274116877217,\n",
    "            'min_split_gain':0.02817555628928521,\n",
    "            'min_child_weight': 39.64402829035144,\n",
    "            'silent': -1,\n",
    "            'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "1: application train & test ( Wed Aug 29 12:43:18 2018 )\n",
      "--------------------\n",
      "The dataset application_train has the shape (307511, 122)\n",
      "Memory usage of dataframe: 325.13 MB\n",
      "Memory usage after optimization: 85.27 MB\n",
      "Decreased by 73.8%\n",
      "     DF shape: (356244, 130)\n",
      "--------------------\n",
      "2: bureau & balance ( Wed Aug 29 12:43:35 2018 )\n",
      "--------------------\n",
      "The dataset bureau_balance has the shape (27299925, 3)\n",
      "The dataset bureau has the shape (1716428, 17)\n",
      "Memory usage of dataframe: 477.71 MB\n",
      "Memory usage after optimization: 247.02 MB\n",
      "Decreased by 48.3%\n",
      "     DF shape: (356244, 383)\n",
      "--------------------\n",
      "3: previous_application ( Wed Aug 29 12:44:30 2018 )\n",
      "--------------------\n",
      "Memory usage of dataframe: 471.48 MB\n",
      "Memory usage after optimization: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "The dataset previous_application has the shape (1670214, 37)\n",
      "Memory usage of dataframe: 1591.24 MB\n",
      "Memory usage after optimization: 991.78 MB\n",
      "Decreased by 37.7%\n",
      "     DF shape: (356244, 1330)\n",
      "--------------------\n",
      "4: POS_CASH_balance ( Wed Aug 29 12:48:36 2018 )\n",
      "--------------------\n",
      "Memory usage of dataframe: 610.43 MB\n",
      "Memory usage after optimization: 238.45 MB\n",
      "Decreased by 60.9%\n",
      "The dataset pos_cash_balance has the shape (10001358, 8)\n",
      "Memory usage of dataframe: 75.26 MB\n",
      "Memory usage after optimization: 27.02 MB\n",
      "Decreased by 64.1%\n",
      "     DF shape: (356244, 1367)\n",
      "--------------------\n",
      "5: installments_payments ( Wed Aug 29 12:49:09 2018 )\n",
      "--------------------\n",
      "Memory usage of dataframe: 830.41 MB\n",
      "Memory usage after optimization: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "The datset installments_payments has the shape (13605401, 8)\n",
      "Memory usage of dataframe: 130.84 MB\n",
      "Memory usage after optimization: 104.93 MB\n",
      "Decreased by 19.8%\n",
      "     DF shape: (356244, 1446)\n",
      "--------------------\n",
      "6: credit_card_balance ( Wed Aug 29 12:50:26 2018 )\n",
      "--------------------\n",
      "Memory usage of dataframe: 673.88 MB\n",
      "Memory usage after optimization: 289.33 MB\n",
      "Decreased by 57.1%\n",
      "The dataset credit_card_balance has the shape (3840312, 23)\n",
      "Memory usage of dataframe: 291.15 MB\n",
      "Memory usage after optimization: 225.96 MB\n",
      "Decreased by 22.4%\n",
      "     DF shape: (356244, 2077)\n",
      "--------------------\n",
      "7: final dataset ( Wed Aug 29 12:53:12 2018 )\n",
      "--------------------\n",
      "Memory usage of dataframe: 2328.92 MB\n",
      "Memory usage after optimization: 2318.73 MB\n",
      "Decreased by 0.4%\n"
     ]
    }
   ],
   "source": [
    "#Aggregate all dataset together\n",
    "full_data = aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing empty features there are 2047 features\n",
      "After removing features with the same distribution on 0 and 1 classes there are 1613 features\n",
      "After removing features with not the same distribution on train and test datasets there are 1612 features\n",
      "After removing features not interesting for classifier there are 1574 features\n"
     ]
    }
   ],
   "source": [
    "#clean the dataset\n",
    "full_data = clean_data(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def full_clf_pred(df):\n",
    "  train_df = df[df['TARGET'].notnull()]\n",
    "  test_df = df[df['TARGET'].isnull()]\n",
    "  full_clf = LGBMClassifier(**lgbm_params)\n",
    "  feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "  print(\"start to train\")\n",
    "  full_clf.fit(train_df[feats], train_df[\"TARGET\"])\n",
    "  prediction = full_clf.predict_proba(train_df[feats])[:,1]\n",
    "  print(\"AUC of training data is {}\".format(roc_auc_score(train_df[\"TARGET\"], prediction)))\n",
    "  test_prediction = full_clf.predict_proba(test_df[feats])[:,1]\n",
    "  del train_df, test_df\n",
    "  gc.collect()\n",
    "  return(test_prediction)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (307500, 1574), test shape: (48744, 1574)\n",
      "Fold 0 started at Tue Aug 28 22:24:49 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1413]\ttraining's auc: 0.889966\tvalid_1's auc: 0.789092\n",
      "Fold  0 AUC : 0.789061\n",
      "Fold 1 started at Tue Aug 28 22:47:56 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1393]\ttraining's auc: 0.888319\tvalid_1's auc: 0.79128\n",
      "Fold  1 AUC : 0.791280\n",
      "Fold 2 started at Tue Aug 28 23:10:01 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2063]\ttraining's auc: 0.915786\tvalid_1's auc: 0.786327\n",
      "Fold  2 AUC : 0.786336\n",
      "Fold 3 started at Tue Aug 28 23:37:39 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1642]\ttraining's auc: 0.898465\tvalid_1's auc: 0.79132\n",
      "Fold  3 AUC : 0.791320\n",
      "Fold 4 started at Wed Aug 29 00:01:04 2018\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1379]\ttraining's auc: 0.888147\tvalid_1's auc: 0.788582\n",
      "Fold  4 AUC : 0.788580\n",
      "Full AUC score 0.789242\n"
     ]
    }
   ],
   "source": [
    "#use lightgbm classifier with 5 folds cross validation method\n",
    "result = cv_scores(full_data, 5, lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe4fb091c39f4edd0bbf997bcc5630e35b47228b"
   },
   "source": [
    "### Hyper parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "bba1ec6f8d9f45f0c0b0d5c927df6f8b893c4560"
   },
   "outputs": [],
   "source": [
    "#parameters obtained using bayesian optimization\n",
    "lgbm_params = {'colsample_bytree': 0.805415337217056,\n",
    " 'learning_rate': 0.011723218430046927,\n",
    " 'max_depth': 8,\n",
    " 'min_child_weight': 39.64402829035144,\n",
    " 'min_split_gain': 0.02817555628928521,\n",
    " 'num_leaves': 33,\n",
    " 'reg_alpha': 0.04988371263522236,\n",
    " 'reg_lambda': 0.0793274116877217,\n",
    " 'subsample': 0.8082978617542438}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9cd32bd6d9279b1575d87877790dc235283903a8"
   },
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "64a682f66238fa787178ee8302ea5273c8646555"
   },
   "outputs": [],
   "source": [
    "#The function below is the LightGBM with 2 fold cross validation\n",
    "#We use bayesian optimization to optimize the hyper parameters of this function\n",
    "def lgbm_evaluate(**params):\n",
    "    \"\"\"\n",
    "    **params : parameters passed to the LightGBM classifier\n",
    "    \n",
    "    The function returns the AUC on training dataset\n",
    "    \"\"\"\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "        \n",
    "    clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n",
    "\n",
    "    train_df = full_data[full_data['TARGET'].notnull()]\n",
    "    test_df = full_data[full_data['TARGET'].isnull()]\n",
    "\n",
    "    folds = KFold(n_splits = 2, shuffle = True, random_state = 1001)\n",
    "        \n",
    "    test_pred_proba = np.zeros(train_df.shape[0])\n",
    "    \n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = False, early_stopping_rounds = 100)\n",
    "\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(train_df['TARGET'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "556c867c985588834ef080975c6be88a5c54cd90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   learning_rate |   max_depth |   min_child_weight |   min_split_gain |   num_leaves |   reg_alpha |   reg_lambda |   subsample | \n"
     ]
    }
   ],
   "source": [
    "#Initial setup of hyper parameter ranges\n",
    "params = {'colsample_bytree': (0.8, 1),\n",
    "          'learning_rate': (.01, .025), \n",
    "          'num_leaves': (25, 35), \n",
    "          'subsample': (0.8, 1), \n",
    "          'max_depth': (6, 9), \n",
    "          'reg_alpha': (.03, .05), \n",
    "          'reg_lambda': (.06, .08), \n",
    "          'min_split_gain': (.01, .03),\n",
    "          'min_child_weight': (38, 40)}\n",
    "#Bayesian optimization to select hyper parameters\n",
    "bo = BayesianOptimization(lgbm_evaluate, params)\n",
    "bo.maximize(init_points = 5, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "e79d57053f66ddc8c166e705514030870bc67d0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.805415337217056,\n",
       " 'learning_rate': 0.011723218430046927,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 39.64402829035144,\n",
       " 'min_split_gain': 0.02817555628928521,\n",
       " 'num_leaves': 33,\n",
       " 'reg_alpha': 0.04988371263522236,\n",
       " 'reg_lambda': 0.0793274116877217,\n",
       " 'subsample': 0.8082978617542438}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out the best parameters\n",
    "best_params = bo.res['max']['max_params']\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "\n",
    "best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
